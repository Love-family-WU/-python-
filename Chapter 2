#1.学习：就是确定合适参数的过程，人需要做的是构建感知机的结构并把训练数据交给计算机
#2.感知机：一个拥有输入和输出的算法，输入信号和输出信号均为0 or 1，(经过后续学习会发现不是必要0 或1，此时只是为了更好的引入感知机概念)其存在一定的结构形式，其中权重和阈值(转置)
#是感知机的参数，感知机的层数和输入信号的个数决定了感知机的结构形式，参数决定了感知机的性质
#同一结构，参数不同会构成不同的感知机，体现了感知机的多样性进而贴近人体的神经系统的复杂性-----------第一性原理出发：通过尽可能复制人脑学习逻辑，构建人工智能
#3.表示方式有图像和数学方式
#4.感知机中的数据包括，输入信号X=（x1,x2,.....xn),权重W=（w1,w2,......wn),阈值&（偏置的相反数-d),输出信号y
#5.为什么会出现阈值到偏置的转换，其实意义没有发生变化，都是为了表示神经元是否被激活，其中偏置更适合数学表示
#以下关于感知机数据的生物意义，可辅助理解，但不能完全紧密相连，因为神经网络（感知机）只是受大脑启发而出现的数学结构，相应AGI的实现不是仿制大脑结构而是出现大脑所具有的功能特性
#6.感知机及其数据的意义：权重代表了对应输入信号的重要程度，阈值大小则代表了输出信号的激活难度
#输入信号和输出信号更类似与神经元，输入信号为0，代表该神经元处并没有神经冲动传来，不会有任何影响
#实际是把输入输出只理解为0 或者1是狭隘的，感知机的输入可以为任意值，但是这样从生物层面又该如何映射呢？
#扩展到下一章的神经网络，输入层即代表大脑从外界接受的信息，经过神经元的不断地传播，最终到输出层，输出层更像是最终大脑做出了判断，等待执行
#数据方面的理解，从原来简单的神经元是否激活即（0/1）到输入经过权重和与阈值比较后，判断对该神经元造成的影响，而不同神经元影响的不断积累
#最终出现输出层的结果，阈值转化为偏置后的理解，输出层经过神经元，根据神经元的特性（权重和阈值）会对输入数据造成一定影响从而改变数值
#多层叠加后： 每层都进行非线性变换，使得网络能够拟合高度复杂的模式（远超感知机
#经过多层神经元的不断影响，最终实现输出层的产出
#而学习的本质就是确定神经元的特性的过程，一经训练过后特性固定，就会有局限----除非根据输入层数据类型的不同会有不同的特性才基本接近人类
#权重的正负代表了神经递质的种类，兴奋性和抑制性，数值大小代表了神经递质的多少反应刺激程度
#阈值则代表了静息电位与动作电位的分界限，达到阈值则电信号传输到输出神经元，使其被激活，相应的输出1
#7.数学公式理解：X*W(X,W为向量)+d，展开后就是超平面的线性表示
#8.数学知识：向量的点积，超平面



import numpy as np

#与门
def AND(x1,x2):
    x=np.array([x1,x2])
    w=np.array([0.5,0.5])
    d=-0.7
    if (d+np.sum(w*x))>0:
        return 1
    else:
        return 0

#非与门
def NAND(x1,x2):
    x=np.array([x1,x2])
    w=np.array([-0.5,-0.5])
    d=0.7
    if (d+np.sum(x*w)>0):
        return 1
    else:
        return 0

#或门
def OR(x1,x2):
    x=np.array([x1,x2])
    w=np.array([0.5,0.5])
    d=-0.2
    if (d+np.sum(x*w)>0):
        return 1
    else :
        return 0
#以上是单层感知机的逻辑表示，单层感知机按照线性来分割空间，因此单层感知机的线性表示往往可以看作一个超平面

#异或门 典型的多层感知机 
def XOR(x1,x2):
    s1=NAND(x1,x2)
    s2=OR(x1,x2)
    return AND(s1,s2)

print(AND(0,1))
print(NAND(0,1))
print(OR(0,1))
print(XOR(0,1))
